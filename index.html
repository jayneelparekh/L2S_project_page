<!DOCTYPE html>
<html lang="en" class="fontawesome-i2svg-active fontawesome-i2svg-complete">
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <script type="text/javascript"
  src="https://www.maths.nottingham.ac.uk/plp/pmadw/LaTeXMathML.js">
  </script>
  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script id="MathJax-script" async
          src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js">
  </script>
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <title>Learning to Steer: Input-dependent Steering for Multimodal LLMs</title>
    <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6">
    </script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js">
    </script>
      <script defer="" src="css/fontawesome.all.min.js"></script>
    <link href="css/bootstrap.min.css" rel="stylesheet">

    <style>
      body {
        font-size:16px;
        margin:80px auto;
        width:auto;
        max-width:850px;
      }

      hr {
        border:0;
        height:1.0px;
        background-image:linear-gradient(to right, rgba(0, 0, 0, 0.3), rgba(0, 0, 0, 0.3), rgba(0, 0, 0, 0.3));
      }

      .gap-30 {
      width:100%;
      height:30px;
      }

      .gap-20 {
      width:100%;
      height:20px;
      }

      .gap-10 {
      width:100%;
      height:10px;
      }

      .gap-5 {
      width:100%;
      height:5px;
      }
      .paper {
        max-width: 700px;
      }
      @media (max-width: 910px) {
        .paper {
          max-width: 500px;
        }
      }
      @media (max-width: 610px) {
        .paper {
          max-width: 300px;
        }
      }
    </style>
  </head>
  
  <body>

    <center><span style="font-size:40px; color:rgb(75, 15, 101)">
        <b> Learning to Steer: Input-dependent Steering <br> for  Multimodal LLMs </b>
    </span></center>
    <div class="gap-20"></div>

    <!---------------------  authors --------------------->
    <div class="row">
        &emsp; &emsp; &emsp; &emsp; &emsp; &emsp; &emsp; &emsp;
        <p>
          <a href="https://jayneelparekh.github.io/"><span style="font-size:24px; color:rgb(172, 138, 30)">Jayneel Parekh*</span></a>
          &emsp; &emsp;
          <a href="https://pegah-kh.github.io/"><span style="font-size:24px; color:rgb(172, 138, 30)">Pegah Khayatan*</span></a>
          &emsp; &emsp;
          <a href="https://mustafashukor.github.io/"><span style="font-size:24px; color:rgb(172, 138, 30)">Mustafa Shukor</span></a>
          &emsp; &emsp;
          <br>
          <a href="https://www.linkedin.com/in/arnaud-dapogny-12653493/"><span style="font-size:24px; color:rgb(172, 138, 30)">Arnaud Dapogny</span></a>
          &emsp; &nbsp;
          <a href="https://sites.google.com/site/alasdairnewson/"><span style="font-size:24px; color:rgb(172, 138, 30)">Alasdair Newson</span></a>
          &emsp; &emsp; &nbsp;
          <a href="https://cord.isir.upmc.fr/"><span style="font-size:24px; color:rgb(172, 138, 30)">Matthieu Cord</span></a>
        </p>
    </div>
    
    <div class="row">
        &emsp; &emsp; &emsp; &emsp; &emsp; &emsp; &emsp; &emsp; &emsp; &emsp; &emsp; &emsp; &emsp;  &emsp;
        <p style="font-size:24px; color:rgb(172, 138, 30)">ISIR, Sorbonne Universit√©, France </p>
    </div>

     <div class="gap-10"></div>

    <center>
        <span style="font-size:21px">
          [<a href="https://arxiv.org/abs/2508.12815" style="color:rgb(75, 15, 101)">Paper</a>]
          &emsp; 
          [<a href="https://github.com/mshukor/xl-vlms" style="color:rgb(75, 15, 101)">Code</a>]&nbsp;
        </span>
    </center>
    
    <div class="gap-20"></div>

    <hr>
    <div class="gap-10"></div>

    <b><span style="font-size:36px; color:rgb(75, 15, 101)">Key Takeaways</span></b>
    <div class="gap-10"></div>
    
    <ul style="font-size:19px">
      <li> This work is on steering for multimodal LLMs (MLLMs). The central theme in our work is that a steering behavior can instantiate itself differently for different inputs. This motivates the need to steer differently depending upon input context.</li>
      <div class="gap-10"></div>
      <li> We first propose <a href="#p2s" style="color:rgb(172, 138, 30)"><b><u>Prompt-to-Steer (P2S)</u></b></a>, a type of contrastive steering method which demonstrates that if the behavior instantiation is known in advance, one can use input specific contrastive prompts to extract and steer the given input directly.</li>
      <div class="gap-10"></div>
      <li> Since P2S requires knowing the instantiation in advance it is not directly useful in practice. We then propose <a href="#l2s" style="color:rgb(172, 138, 30)"><b><u>Learn-to-Steer (L2S)</u></b></a> method that learns to predict input-specific P2S steering vectors from input context. The L2S prediction is used to steer the MLLM.</li>
      <div class="gap-10"></div>
      <li> We apply L2S to steer MLLMs for two different applications: safety enforcement and hallucination mitigation. Compared to the popular mean-steering approach, L2S allows for simultaneous steering across multiple behavior instantiations.</li>
    </ul>


    <div class="gap-20"></div>

    <hr>

    <b><span style="font-size:36px; color:rgb(75, 15, 101)"> 
      Background and Motivation
    </span></b>

    <div class="gap-10"></div>

    <p style="font-size:19px">
    <i>Representation steering</i> refers to the idea of intervening on the internal representations in order to "align" the output with certain desired behaviors. 
       For computer vision enthusiasts, in principle, it's similar to a class of image editing approaches where one intervenes on the latent representations of image generative models to control the output "style".
    </p>

    <p style="font-size:19px">
    For a given desired behavior (eg. sycophancy), most current steering approaches involve contrasting representations between two sets of samples, one that aligns with the given behavior, and the other that doesn't. 
    Subsequently, one extracts mean-of-difference or difference-of-means between the two sets of representations as a single steering vector to induce desired behavior. 
    </p>

    <p style="font-size:19px">
    The core of our proposal is that when steering for a given goal/behavior (eg. ensuring safety), it can realize itself differently for different inputs. 
    For example, OpenAI usage policy provides a set of unsafe scenarios where a model response should not be used/relied upon. 
    One could steer so that the model refuses a user query regardless of the scenario, which would ensure a safe response. However, safety can mean different things depending upon input context.  
    It would entail refusal for queries asking about performing harmful/illegal activities. 
    For queries about legal/financial/healthcare consultations, often what is more essential is not that the model refuses to answer them but that it should recommend (atleast at some point) to defer the user to consult a human expert (lawyer/doctor etc.)
    This can indeed be regarded as a subjective opinion about how a model should behave, and that is precisely the point!
    This in turn motivated us to propose methods that can steer differently for a behavior, taking into account the input context. 
    </p>

    <div class="gap-20"></div>

    <hr>

    <b><span style="font-size:36px; color:rgb(75, 15, 101)"> 
      Method
    </span></b>

    <center>
    <video width="810" height="480" autoplay loop muted>
    <source src="assets/system_design_recording_v3.mp4" type="video/mp4">
    Your browser does not support the video tag.
    </video>
    </center>

    <div id="p2s">
    <b><span style="font-size:24px; color:rgb(172, 138, 30)"> 
    Prompt-to-Steer (P2S): 
    </span></b>
    </div>
   
    <br>

    <p style="font-size:19px">
    For a given multimodal input query \(X=(I, T) \), we define a pair of contrastive prompts, \( (T^+_X, T^-_X) \), that correspond to desired and undesired behaviors respectively. 
    Crucially, unlike previous steering methods, that use a fixed set of prompts for all samples, we allow use of input-specific prompts, aligned with desired behavior instantiation for the given input. 
    These prompt completions are appended to original input to create modified inputs \(X^+ = (I, T || T^+_X)\), \(X^- = (I, T || T^-_X)\). 
    The difference between the final token representations (most often) is extracted at the steering layer \(L^*\) as the input-specific steering vector \(z_{X, L^*}\).

    To steer the given input one can simply apply \(z_{X, L^*}\) to shift the hidden representations at layer \(L^*\).
    </p>

    <center style="font-size:19px">
      \(h_{L^*}^p(X) \leftarrow h_{L^*}^p(X) + \alpha z_{X, L^*}\)
    </center>

    <br>
    <p style="font-size:19px">
    Importantly, if we know the behavior instantiation in advance, P2S uses only the sample and input-specific prompt completions to extract a steering vector the given input
    Example of such prompt completions are illustrated 
    </p>


    <div id="l2s">
    <b><span style="font-size:24px; color:rgb(172, 138, 30)"> 
    Learn-to-Steer (P2S): 
    </span></b>
    </div>
   
    <br>

    <p style="font-size:19px">
    
    </p>


    <br>
    <p style="font-size:19px">
   
    </p>
    


  </body>
  
</html>
